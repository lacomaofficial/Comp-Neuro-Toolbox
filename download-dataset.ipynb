{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-05T17:55:13.928279Z",
     "iopub.status.busy": "2024-10-05T17:55:13.927837Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from nilearn.maskers import MultiNiftiMapsMasker\n",
    "from nilearn.connectome import ConnectivityMeasure\n",
    "from nilearn import datasets\n",
    "\n",
    "# Define the directory where the data will be saved\n",
    "save_directory = \"D:\\\\ads000030\"\n",
    "\n",
    "\n",
    "# Fetch the dataset from openneuro with the filtering options\n",
    "dataset = datasets.fetch_openneuro_dataset(urls=None, data_dir=save_directory, dataset_version='ds000030_R1.0.5', verbose=1)[source]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "len(dataset.func_preproc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from nilearn import datasets\n",
    "\n",
    "# Atlas \n",
    "#setup\n",
    "dim = 64\n",
    "difumo = datasets.fetch_atlas_difumo(dimension=dim, resolution_mm=2, legacy_format=False)\n",
    "atlas_filename = difumo.maps\n",
    "\n",
    "\n",
    "\n",
    "# create masker using MultiNiftiMapsMasker to extract functional data within\n",
    "# atlas parcels from multiple subjects using parallelization to speed up the\n",
    "# # computation\n",
    "masker = MultiNiftiMapsMasker(\n",
    "    maps_img=atlas_filename,\n",
    "    standardize=True,  # Standardize to improve comparisons between subjects\n",
    "    n_jobs=-1,  # Parallelize over all available CPUs\n",
    "    verbose=0\n",
    ")\n",
    "\n",
    "# ConenctivityMeasure from Nilearn uses simple 'correlation' to compute\n",
    "# connectivity matrices for all subjects in a list\n",
    "connectome_measure = ConnectivityMeasure(\n",
    "    kind='correlation', vectorize=True, discard_diagonal=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Convert the dataset to a DataFrame\n",
    "pheno = pd.DataFrame(dataset.phenotypic)\n",
    "\n",
    "# Save the DataFrame as a CSV file in the correct directory\n",
    "pheno.to_csv('/kaggle/working/schizo_pheno.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "pheno.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "func_preproc_files = dataset.func_preproc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Check if the file was saved successfully\n",
    "import os\n",
    "print(\"Files in the working directory:\", os.listdir('/kaggle/working/'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "all_features = []\n",
    "for i, sub in enumerate(func_preproc_files):\n",
    "    print(f\"Processing subject {i+1}...\")\n",
    "    \n",
    "    # Step 1: Fit transform with masker\n",
    "    print(f\"  Fitting and transforming masker for subject {i+1}...\")\n",
    "    masked_data = masker.fit_transform(sub)\n",
    "    \n",
    "    # Step 2: Fit transform with connectome_measure\n",
    "    print(f\"  Fitting and transforming connectome measure for subject {i+1}...\")\n",
    "    transformed_data = connectome_measure.fit_transform([masked_data])[0]\n",
    "    \n",
    "    # Step 3: Append to all_features\n",
    "    print(f\"  Appending features for subject {i+1}...\")\n",
    "    all_features.append(transformed_data)\n",
    "    \n",
    "    # Print statement after completing processing for the subject\n",
    "    print(f\"  Completed processing for subject {i+1}\\n\")\n",
    "\n",
    "print(\"All subjects processed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Let's save the data to disk\n",
    "\n",
    "np.savez_compressed('Autism_Classf_featuress.npz',a = all_features)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 5745213,
     "sourceId": 9451887,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30786,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "xtra",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
